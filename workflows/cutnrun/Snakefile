import sys
sys.path.insert(0, srcdir('../..'))
import pandas
import os
from lib import common, utils, chipseq
from lib.patterns_targets import CutnRunConfig

modes = ['spikein', 'endogenous']

if not workflow.overwrite_configfile:
    configfile: 'config/config.yaml'
else:
    configfile: workflow.overwrite_configfile

config = common.load_config(config)

c = CutnRunConfig(
    config,
    config.get('patterns', 'config/cutnrun_patterns.yaml')
)

wildcard_constraints:
    n = '[1,2]'

# low cutoff for lenient peak calling
peak_calling_dict = c.config['cutnrun']['peak_calling'][0]
threshold = peak_calling_dict['threshold']

final_targets = utils.flatten((
    c.targets['fastq'],
    c.targets['bam'],
    utils.flatten(c.targets['fastqc']),
    utils.flatten(c.targets['libsizes']),
    #[c.targets['fastq_screen']],
    #[c.targets['libsizes_table']],
    c.targets['multiqc'],
    c.targets['agg'],
    #utils.flatten(c.targets['markduplicates']),
    utils.flatten(c.targets['bigwig']),
    utils.flatten(c.targets['peaks']),
    #utils.flatten(c.targets['merged_techreps']),
    #utils.flatten(c.targets['fingerprint']),
    utils.flatten(c.targets['bigbed']),
    #utils.flatten(c.targets['multibigwigsummary']),
    #utils.flatten(c.targets['plotcorrelation']),
))

for f in final_targets:
    print(f)

#print(utils.flatten(c.patterns_by_peaks['peaks']))

rule targets:
    input: 
        final_targets,

bowtie2_parameters = {
    'spikein':    "--local --very-sensitive-local --no-mixed --no-discordant --phred33 -I 10 -X 700 --no-overlap --no-dovetail ",
    'endogenous': "--local --very-sensitive-local --no-mixed --no-discordant --phred33 -I 10 -X 700 --no-overlap --no-dovetail ",
}

bowtie2_indexes = {
    'spikein':    "/data/NICHD-core0/references/sacCer/sacCer3/bowtie2/sacCer_sacCer3",
    'endogenous': "/data/NICHD-core0/references/dmel/r6-28/bowtie2/dmel_r6-28",
}

exts = [
    '.1.bt2',
    '.2.bt2',
    '.3.bt2',
    '.4.bt2',
    '.rev.1.bt2',
    '.rev.2.bt2',
]

_st = c.sampletable.set_index(c.sampletable.columns[0])
#_st = c.sampletable
print(_st.index)

# Symlink over raw data into a uniform location
def orig_for_sample(wc):
    #print(wc)
    if c.is_paired:
        return _st.loc[wc.sample, ['orig_filename', 'orig_filename_R2']]
    return _st.loc[wc.sample, 'orig_filename']

def render_r1_r2(pattern):
    return expand(pattern, sample='{sample}',n=c.n)

rule symlinks:
    input: orig_for_sample
    output:
        c.patterns['fastq'],
    wildcard_constraints:
        n="\d+"
    run:
        for src, linkname in zip(input, output):
            utils.make_relative_symlink(src, linkname)

# Run FastQC on raw reads
rule fastqc:
    input:
        #c.patterns['fastq']
        'data/cutnrun_samples/{sample}/{sample}_R1.fastq.gz',
    output:
        #c.patterns['fastq'] + '_fastqc.html',
        #c.patterns['fastq'] + '_fastqc.zip',
        'data/cutnrun_samples/{sample}/fastqc/{sample}_R1_fastqc.html',
        'data/cutnrun_samples/{sample}/fastqc/{sample}_R1_fastqc.zip',
    run:
        #samplename = wildcards.sample
        #print(wc)
        outdir = os.path.dirname(output[0])
        shell(
            'fastqc '
            '--quiet '
            '--extract '
            '{input} '
            '--outdir {outdir} '
        )


# Map reads with Bowtie2
rule bowtie2:
    input:
        fq=render_r1_r2(c.patterns['fastq']),
        index=lambda wildcards: expand(bowtie2_indexes[wildcards.mode] + '{ext}', ext=exts)
    output:
        sam=temp(c.patterns['bam'] + '.sam'),
        fq='data/cutnrun_unmapped/{sample}/{sample}.unmapped.{mode}.fastq.1.gz'
    log:
        c.patterns['bam'] + '.sam' + '.log'
    # NOTE:
    # When running on a cluster, number of threads here should match the
    # resources requested in the cluster config.
    threads: 8
    run:
        index = input.index[0].replace('.1.bt2', '')
        bowtie2_params = bowtie2_parameters[wildcards.mode]
        unmapped = output.fq.replace('.1','')
        shell(
            "bowtie2 "
            "-x {index} "
            "-1 {input.fq[0]} "
            "-2 {input.fq[1]} "
            "--threads {threads} "
            "{bowtie2_params} "
            "-S {output.sam} "
            "--un-conc-gz {unmapped} "
            "2> {log}; "
        )

print(rules.bowtie2.output.sam)
print(c.targets['bam'])

# Remove unmapped reads and sort results
rule samtools_fix:
    input:
        rules.bowtie2.output.sam,
    output:
        bam=c.patterns['bam'],
    run:
        shell(
            "samtools view -Sb -F 0x04 {input} "
            "> {output}.tmp.bam "
            
            # sort bam and clean up
            "&& samtools sort "
            "-o {output} "
            "-O BAM "
            "{output}.tmp.bam "
            "&& rm {output}.tmp.bam "
            )


# Count mapped reads in BAM (will include multimappers)
rule count_reads:
    input:
        c.patterns['bam'],
    output:
        temp('data/cutnrun_mapped/{sample}/{sample}.mapped.{mode}.libsize.txt')
    shell:
        "samtools view -c {input} > {output}"


# Combine the detected bowtie2 results across modes
rule merge_counts:
    input:
        expand('data/cutnrun_mapped/{sample}/{sample}.mapped.{mode}.libsize.txt',
               sample=_st.index, mode=modes),
    output:
        utils.flatten(c.targets['libsizes']),
    run:
        df = []
        for fn in sorted(input):
            toks = os.path.basename(fn).split('.')
            sample = toks[0]
            mode = toks[2]
            d = dict(sample=sample, mode=mode, count=int(open(fn).read().strip()))
            df.append(d)
        df = pandas.DataFrame(df)
        df.to_csv(output[0], sep='\t', index=False)

#print(rules.fastqc.output)

rule multiqc:
    input:
        files=(
            utils.flatten(c.targets['bam']) +
            utils.flatten(c.targets['fastqc']) +
            utils.flatten(c.targets['peaks'])),

        #bam=c.targets['bam'],
        #fastqc=c.targets['fastqc']
        #bam=expand('data/cutnrun_mapped/{sample}.mapped.{{mode}}.bam', sample=sampletable.index),
        #fastqc=expand(rules.fastqc.output, sample=_st.index, n=c.n),
        config='config/multiqc_config.yaml', 
    output:
        html=c.patterns['multiqc'],
        bowtie2='data/cutnrun_merged/multiqc_{mode}_data/multiqc_bowtie2.txt',
    log:
        c.patterns['multiqc'] + '.log'
    run:
        fn = input[0]
        analysis_directory = os.path.dirname(fn) + '/*{0}*'.format(wildcards.mode)
        fastqc = 'data/cutnrun_samples'
        outdir = os.path.dirname(output[0])
        basename = os.path.basename(output[0])
        shell(
            'LC_ALL=en_US.UTF.8 LC_LANG=en_US.UTF-8 '
            'multiqc '
            '--quiet '
            '--outdir {outdir} '
            '--force '
            '--filename {basename} '
            '{analysis_directory} '
            '{fastqc} '
            '2> {log}'
        )

rule aggregate_counts:
    input:
        expand('data/cutnrun_merged/multiqc_{mode}_data/multiqc_bowtie2.txt', mode=modes)
    output:
        c.patterns['agg'],
    run:
        dfs = []
        for i in input:
            mode = i.split('/')[2].split('_')[1]
            df = pandas.read_csv(i, sep='\t', index_col=0)
            cols = df.columns
            cols = [mode + '.' + c for c in cols]
            df.columns = cols
            dfs.append(df)
        full = pandas.concat(dfs, sort=True)
        full.to_csv(output[0], sep='\t')

def get_endogenous(pattern):
    return expand(c.patterns[pattern], sample='{sample}', mode='endogenous')

rule make_bed:
    input:
        get_endogenous('bam')
    output:
        sorted_bed3=temp("data/cutnrun_mapped/{sample}/{sample}.mapped_sorted.endogenous.BED3"),
    threads: 8
    shell:
        'samtools sort -n --threads={threads} {input} > {output.sorted_bed3}.bam'
        '&& bamToBed -bedpe -i {output.sorted_bed3}.bam '
        '| cut -f 1,2,6 '
        '| bedtools sort -i > {output.sorted_bed3}'
        '&& rm {output.sorted_bed3}.bam'

rule index_bam:
    input:
        get_endogenous('bam')
    output:
        bam=get_endogenous('sorted_bam'),
        bai=[bam + '.bai' for bam in get_endogenous('sorted_bam')]
    threads: 8
    run:
        shell('samtools sort --threads={threads} {input} > {output.bam}')
        shell('samtools index {output.bam}')

rule make_norm_bedgraph:
    input:
        bam = rules.index_bam.output.bam,
        bai = rules.index_bam.output.bai,
    output:
        c.patterns['bedgraph']['norm']
    threads: 8
    shell:
        'bamCoverage -b {input.bam} -of bedgraph -o {output} -p {threads} '
        '-bs 5 --normalizeUsing BPM'

rule make_bedgraph:
    input:
        bam = rules.index_bam.output.bam,
        bai = rules.index_bam.output.bai,
    output:
        c.patterns['bedgraph']['all']
    threads: 8
    run:
        shell('bamCoverage -b {input.bam} -of bedgraph -o {output}.tmp -p {threads} -bs 5')
        shell("cat {output}.tmp | awk '{{if($4 > 0) print $0}}' > {output}")
        shell('rm {output}.tmp')

rule run_seacr:
    input:
        lambda wc: expand(c.patterns['bedgraph']['all'], 
                sample=chipseq.samples_for_run(config, wc.seacr_run, 'seacr','ip'))
    output: 
        utils.flatten(c.patterns['peaks']),
    run:
        outdir = os.path.dirname(output[0])
        shell("bash SEACR/SEACR_1.0.sh {input} {threshold} non AUC {outdir}/peaks")
        shell("mv {outdir}/peaks.auc.threshold.merge.bed {outdir}/peaks.bed")

rule make_bigwigs:
    input:
        bam = rules.index_bam.output.bam,
        bai = rules.index_bam.output.bai,
    output:
        c.patterns['bigwig']
    threads: 8
    shell:
        'bamCoverage -b {input.bam} -of bigwig -o {output} -p {threads} -bs 5 --normalizeUsing BPM'

rule make_chrom_sizes:
    input: 
        rules.make_bedgraph.output,
    output:
        "data/cutnrun_mapped/{sample}/{sample}.chrom.sizes",
    run:
        o = open(output[0], 'w')

        chrom = None
        end = None
        
        with open(input[0]) as f:
            for line in f:
                cols = line.strip().split('\t')
                if not chrom:
                    chrom = cols[0]
                    end = cols[2]
                elif chrom != cols[0]:
                    o.write(chrom + '\t' + end + '\n')
                    chrom = cols[0]
                    end = cols[2]
                else:
                    end = cols[2]
        o.write(chrom + '\t' + end + '\n')
        o.close()
                

rule make_bigbeds:
    input:
        bed = utils.flatten(c.patterns['peaks']),
        chrom_sizes = lambda wc: expand(rules.make_chrom_sizes.output, 
                sample=chipseq.samples_for_run(config, wc.seacr_run, 'seacr', 'ip')),
        as_file="seacrbeds.as"
    output:
        utils.flatten(c.patterns['bigbed'])
    shell:
        "bedToBigBed -as={input.as_file} -type=bed3+3 {input.bed} {input.chrom_sizes} {output}"

# vim: ft=python
